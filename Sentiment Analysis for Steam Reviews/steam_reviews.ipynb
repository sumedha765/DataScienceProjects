{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-2-a3d2d2317c6c>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mnltk\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mword_tokenize\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpos_tag\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mne_chunk\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mstring\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mwarnings\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mnltk\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mre\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "import string\n",
    "import warnings\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.linear_model import RidgeClassifier,LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"Data/train.csv\")\n",
    "df_game = pd.read_csv(\"Data/game_overview.csv\")\n",
    "df_test = pd.read_csv(\"Data/test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(df_train.head())\n",
    "print(df_test.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Number of rows are\", df_train.shape[0], \".Number of columns is\", df_train.shape[1])\n",
    "print(\"Number of rows are\", df_test.shape[0], \".Number of columns is\", df_test.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Data Type of each column.Return Object.\n",
    "print(\"Dataframe data types\")\n",
    "print(df_train.dtypes)\n",
    "print(df_test.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Return column names as Index object.\n",
    "print(\"Dataframe column data types\")\n",
    "print(df_train.columns)\n",
    "print(df_test.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Class distribution of target available.\n",
    "print(\"Distribution of target variable\")\n",
    "print(df_train.user_suggestion.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Class distribution of title variable.\n",
    "print(\"Distribution of title variable\")\n",
    "print(df_train.title.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Converting all letters to lowercase\")\n",
    "# Convert text to lowercase.\n",
    "def tolowercase(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "df_train['user_review'] = df_train.user_review.apply(tolowercase)\n",
    "df_test['user_review'] = df_test.user_review.apply(tolowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Removing punctuation\")\n",
    "\n",
    "# Remove punctuation.\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "df_train['user_review'] = df_train.user_review.apply(remove_punctuation)\n",
    "df_test['user_review'] = df_test.user_review.apply(remove_punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Perform lemmatization\")\n",
    "\n",
    "# Lemmatization:\n",
    "def do_lemmatization(text):\n",
    "    lemma_words = set([])\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = word_tokenize(text)\n",
    "    for word in text:\n",
    "        lemma_words.add(lemmatizer.lemmatize(word))\n",
    "    return \" \".join(lemma_words)\n",
    "\n",
    "df_train['user_review'] = df_train.user_review.apply(do_lemmatization)\n",
    "df_test['user_review'] = df_test.user_review.apply(do_lemmatization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Perform named entity recognition\")\n",
    "\n",
    "def named_entity_recognition(text):\n",
    "    result = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "    return result\n",
    "\n",
    "text_1 = df_train.user_review[0]\n",
    "ner = named_entity_recognition(text_1)\n",
    "print(ner)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Part-of-speech tagging using NLTK\")\n",
    "\n",
    "def pos_tagging(text):\n",
    "    text = word_tokenize(text)\n",
    "    tokens_tag = pos_tag(text)\n",
    "    return tokens_tag\n",
    "\n",
    "df_train['pos_tagging'] = df_train.user_review.apply(pos_tagging)\n",
    "print(df_train['pos_tagging'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# NUMBER OF 15 MOST FREQUENT TERMS.\n",
    "token = nltk.word_tokenize(''.join(df_train.user_review))\n",
    "frequent = nltk.FreqDist(token)\n",
    "print(frequent.most_common(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Text with highest number of words.\n",
    "df_train['number_of_words'] = df_train['user_review'].apply(lambda x: len(str(x).split()))\n",
    "print('Maximum number of word',df_train['number_of_words'].max())\n",
    "print('\\nSentence:\\n',df_train[df_train['number_of_words'] == 587]['user_review'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = df_train['user_review']\n",
    "y = df_train['user_suggestion']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Feature Extraction using TFIDF-Char Based.\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,5), stop_words='english', analyzer='char',max_features=5000)\n",
    "print(type(tfidf_vec))  # TfidfVectorizer class.\n",
    "train_tfidf_vec = tfidf_vec.fit_transform(X_train)\n",
    "print(type(train_tfidf_vec))  # Sparse CSR matrix.\n",
    "valid_tfidf_vec = tfidf_vec.transform(X_test)\n",
    "print(type(valid_tfidf_vec))  # Sparse CSR matrix.\n",
    "test_tfidf_vec = tfidf_vec.transform(df_test['user_review'])\n",
    "train_vector_array = train_tfidf_vec.toarray()\n",
    "valid_vector_array = valid_tfidf_vec.toarray()\n",
    "test_vector_array = test_tfidf_vec.toarray()\n",
    "\n",
    "print(train_vector_array.shape)\n",
    "print(valid_vector_array.shape)\n",
    "print(test_vector_array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Linear support vector classifier.\n",
    "lsvc = LinearSVC(C=1,loss= 'hinge',random_state=999)\n",
    "lsvc.fit(train_vector_array, y_train)\n",
    "y_pred = lsvc.predict(valid_vector_array)\n",
    "print(f1_score(y_test,y_pred,average='micro'))   # 0.8250\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Logistic Regression.\n",
    "lr = LogisticRegression(random_state=999)\n",
    "lr.fit(train_vector_array, y_train)\n",
    "y_pred = lr.predict(valid_vector_array)\n",
    "print(f1_score(y_test,y_pred,average='micro'))  # 0.8128\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Random Forest Classifier.\n",
    "rfc = RandomForestClassifier(n_estimators=300, random_state=999)\n",
    "rfc.fit(train_vector_array, y_train)\n",
    "pred = rfc.predict(valid_vector_array)\n",
    "print(f1_score(y_test, pred))  # 0.8270\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Bernoulli Naive Bayes.\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(train_vector_array,y_train)\n",
    "y_pred = bnb.predict(valid_vector_array)\n",
    "print(f1_score(y_test,y_pred,average='micro'))  # 0.6547\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Ridge Classifier.\n",
    "ridge = RidgeClassifier(random_state=999)\n",
    "ridge.fit(train_vector_array,y_train)\n",
    "y_pred = ridge.predict(valid_vector_array)\n",
    "print(f1_score(y_test,y_pred,average='micro'))  # 0.8348\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pred_final = ridge.predict((test_vector_array))\n",
    "df = pd.DataFrame({'review_id': df_test['review_id'], 'user_suggestion': pred_final})\n",
    "df.to_csv(\"submission_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# LDA Topic Modelling:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(max_df=0.8, min_df=2, stop_words='english')\n",
    "doc_term_matrix = count_vect.fit_transform(X_train)\n",
    "print(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Each of x documents is represented as y dimensional vector,which means that our vocabulary has y words.\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "LDA = LatentDirichletAllocation(n_components=5,\n",
    "                                random_state=0)\n",
    "print(type(LDA))  # LatentDirichletAllocation class.\n",
    "LDA.fit(doc_term_matrix)\n",
    "print(LDA.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# For each topic,each word of the document is assigned a weight.\n",
    "# Higher weight means it is the top word of the topic.\n",
    "# It is a multidimensional array.Each row represent the topic,each column represents the word in a document.\n",
    "# Shape = [n_topics,n_words] or [n_components, n_features]\n",
    "\n",
    "# Define helper function to print top words for each topic.\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for index, topic in enumerate(model.components_):\n",
    "        print(topic)\n",
    "        message = \"\\nTopic #{}:\".format(index)\n",
    "        print(message)\n",
    "        print([(feature_names[i], topic[i]) for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        # feature_names[i] is a word,topic[i] is the weight of the word for that topic.\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "number_of_words = 50\n",
    "print(\"\\nTopics in LDA model: \")\n",
    "tf_feature_names = count_vect.get_feature_names()\n",
    "print_top_words(LDA, tf_feature_names, number_of_words)\n",
    "#####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Topic Modelling : NMF: Non-Matrix factorization.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer(max_df=0.8, min_df=2, stop_words='english')\n",
    "doc_term_matrix = tfidf_vect.fit_transform(X_train)\n",
    "print(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf = NMF(n_components=5, random_state=42)\n",
    "print(type(nmf))\n",
    "nmf.fit(doc_term_matrix)\n",
    "print(nmf.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # For each topic,each word of the document is assigned a weight.\n",
    "# # Higher weight means it is the top word of the topic.\n",
    "# # It is a multidimensional array.Each row represent the topic,each column represents the word in a document.\n",
    "# Shape - [n_topics,n_words] or [n_components, n_features].\n",
    "# Factorization matrix.\n",
    "\n",
    "# Define helper function to print top words.\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for index, topic in enumerate(model.components_):\n",
    "        message = \"\\nTopic #{}:\".format(index)\n",
    "        print(message)\n",
    "        print([(feature_names[i], topic[i]) for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "number_of_words = 50\n",
    "print(\"\\nTopics in NMF model: \")\n",
    "tf_feature_names = tfidf_vect.get_feature_names()  # note that tf_vectorizer is an LemmaCountVectorizer object and with this command we get the whole dictionary of words\n",
    "print_top_words(nmf, tf_feature_names, number_of_words)\n",
    "\n",
    "######################################################\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}