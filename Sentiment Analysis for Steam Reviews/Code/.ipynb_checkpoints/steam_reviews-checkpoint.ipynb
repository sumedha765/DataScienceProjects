{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "import string\n",
    "import warnings\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.linear_model import RidgeClassifier,LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Steam is a video game digital distribution service with a vast community of gamers globally. A lot of gamers write reviews at the game page and have an option of choosing whether they would recommend this game to others or not. However, determining this sentiment automatically from text can help Steam to automatically tag such reviews extracted from other forums across the internet and can help them better judge the popularity of games.Given the review text with user recommendation and other information related to each game for 64 game titles, the task is to predict whether the reviewer recommended the game titles available in the test set on the basis of review text and other information. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Steam is a video game digital distribution service with a vast community of gamers globally.\n",
    "A lot of gamers write reviews at the game page and have an option of choosing whether they would recommend this game to others or not.\n",
    "However, determining this sentiment automatically from text can help Steam to automatically tag such reviews extracted from other forums across the internet and can help them better judge the popularity of games.Given the review text with user recommendation and other information related to each game for 64 game titles, the task is to predict whether the reviewer recommended the game titles available in the test set on the basis of review text and other information. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Reading data source files.\n",
    "\n",
    "df_train = pd.read_csv(\"../Data/train.csv\")\n",
    "df_game = pd.read_csv(\"../Data/game_overview.csv\")\n",
    "df_test = pd.read_csv(\"../Data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows are 17494 .Number of columns is 5\n",
      "Number of rows are 8045 .Number of columns is 4\n",
      "Number of rows are 64 .Number of columns is 5\n"
     ]
    }
   ],
   "source": [
    "# Checking training and testing data size.\n",
    "print(\"Number of rows are\", df_train.shape[0], \".Number of columns is\", df_train.shape[1])\n",
    "print(\"Number of rows are\", df_test.shape[0], \".Number of columns is\", df_test.shape[1])\n",
    "print(\"Number of rows are\", df_game.shape[0], \".Number of columns is\", df_game.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging data.\n",
    "\n",
    "df_train= df_train.merge(df_game, how='left', on='title')\n",
    "df_test= df_test.merge(df_game, how='left', on='title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   review_id                        title    year  \\\n",
      "0          1  Spooky's Jump Scare Mansion  2016.0   \n",
      "1          2  Spooky's Jump Scare Mansion  2016.0   \n",
      "2          3  Spooky's Jump Scare Mansion  2016.0   \n",
      "3          4  Spooky's Jump Scare Mansion  2015.0   \n",
      "4          5  Spooky's Jump Scare Mansion  2015.0   \n",
      "5          6  Spooky's Jump Scare Mansion  2015.0   \n",
      "6          7  Spooky's Jump Scare Mansion  2017.0   \n",
      "7          8  Spooky's Jump Scare Mansion  2015.0   \n",
      "8          9  Spooky's Jump Scare Mansion  2015.0   \n",
      "9         10  Spooky's Jump Scare Mansion  2015.0   \n",
      "\n",
      "                                         user_review  user_suggestion  \\\n",
      "0  I'm scared and hearing creepy voices.  So I'll...                1   \n",
      "1  Best game, more better than Sam Pepper's YouTu...                1   \n",
      "2  A littly iffy on the controls, but once you kn...                1   \n",
      "3  Great game, fun and colorful and all that.A si...                1   \n",
      "4  Not many games have the cute tag right next to...                1   \n",
      "5  Early Access ReviewIt's pretty cute at first, ...                1   \n",
      "6  Great game. it's a cute little horror game tha...                1   \n",
      "7  Spooky's Jump Scare Mansion is a Free Retro ma...                1   \n",
      "8  Somewhere between light hearted, happy parody ...                0   \n",
      "9  This game with its cute little out of the wall...                1   \n",
      "\n",
      "      developer     publisher  \\\n",
      "0  Lag Studios   Lag Studios    \n",
      "1  Lag Studios   Lag Studios    \n",
      "2  Lag Studios   Lag Studios    \n",
      "3  Lag Studios   Lag Studios    \n",
      "4  Lag Studios   Lag Studios    \n",
      "5  Lag Studios   Lag Studios    \n",
      "6  Lag Studios   Lag Studios    \n",
      "7  Lag Studios   Lag Studios    \n",
      "8  Lag Studios   Lag Studios    \n",
      "9  Lag Studios   Lag Studios    \n",
      "\n",
      "                                                tags  \\\n",
      "0  ['Horror', 'Free to Play', 'Cute', 'First-Pers...   \n",
      "1  ['Horror', 'Free to Play', 'Cute', 'First-Pers...   \n",
      "2  ['Horror', 'Free to Play', 'Cute', 'First-Pers...   \n",
      "3  ['Horror', 'Free to Play', 'Cute', 'First-Pers...   \n",
      "4  ['Horror', 'Free to Play', 'Cute', 'First-Pers...   \n",
      "5  ['Horror', 'Free to Play', 'Cute', 'First-Pers...   \n",
      "6  ['Horror', 'Free to Play', 'Cute', 'First-Pers...   \n",
      "7  ['Horror', 'Free to Play', 'Cute', 'First-Pers...   \n",
      "8  ['Horror', 'Free to Play', 'Cute', 'First-Pers...   \n",
      "9  ['Horror', 'Free to Play', 'Cute', 'First-Pers...   \n",
      "\n",
      "                                            overview  \n",
      "0  Can you survive 1000 rooms of cute terror? Or ...  \n",
      "1  Can you survive 1000 rooms of cute terror? Or ...  \n",
      "2  Can you survive 1000 rooms of cute terror? Or ...  \n",
      "3  Can you survive 1000 rooms of cute terror? Or ...  \n",
      "4  Can you survive 1000 rooms of cute terror? Or ...  \n",
      "5  Can you survive 1000 rooms of cute terror? Or ...  \n",
      "6  Can you survive 1000 rooms of cute terror? Or ...  \n",
      "7  Can you survive 1000 rooms of cute terror? Or ...  \n",
      "8  Can you survive 1000 rooms of cute terror? Or ...  \n",
      "9  Can you survive 1000 rooms of cute terror? Or ...  \n"
     ]
    }
   ],
   "source": [
    "# Printing first 10 rows of the data.\n",
    "\n",
    "print(df_train.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_id            int64\n",
      "title               object\n",
      "year               float64\n",
      "user_review         object\n",
      "user_suggestion      int64\n",
      "developer           object\n",
      "publisher           object\n",
      "tags                object\n",
      "overview            object\n",
      "dtype: object\n",
      "review_id        int64\n",
      "title           object\n",
      "year           float64\n",
      "user_review     object\n",
      "developer       object\n",
      "publisher       object\n",
      "tags            object\n",
      "overview        object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Checking column datatypes.\n",
    "\n",
    "print(df_train.dtypes)\n",
    "print(df_test.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted all letters to lowercase.\n"
     ]
    }
   ],
   "source": [
    "print(\"Converted all letters to lowercase.\")\n",
    "\n",
    "# Convert text to lowercase.\n",
    "def tolowercase(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "df_train['user_review'] = df_train.user_review.apply(tolowercase)\n",
    "df_test['user_review'] = df_test.user_review.apply(tolowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed punctuation\n"
     ]
    }
   ],
   "source": [
    "print(\"Removed punctuation\")\n",
    "\n",
    "# Remove punctuation.\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "df_train['user_review'] = df_train.user_review.apply(remove_punctuation)\n",
    "df_test['user_review'] = df_test.user_review.apply(remove_punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Stopwords\n"
     ]
    }
   ],
   "source": [
    "print(\"Removed Stopwords\")\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    all_stopwords = stopwords.words('english')\n",
    "    text_tokens = word_tokenize(text)\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\n",
    "    return \" \".join(tokens_without_sw)\n",
    "\n",
    "df_train['user_review'] = df_train.user_review.apply(remove_stopwords)\n",
    "df_test['user_review'] = df_test.user_review.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed lemmatization\n"
     ]
    }
   ],
   "source": [
    "print(\"Performed lemmatization\")\n",
    "\n",
    "# Perform Lemmatization.\n",
    "def do_lemmatization(text):\n",
    "    lemma_words = set([])\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = word_tokenize(text)\n",
    "    for word in text:\n",
    "        lemma_words.add(lemmatizer.lemmatize(word))\n",
    "    return \" \".join(lemma_words)\n",
    "\n",
    "df_train['user_review'] = df_train.user_review.apply(do_lemmatization)\n",
    "df_test['user_review'] = df_test.user_review.apply(do_lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed Named Entity Recognition\n",
      "(S\n",
      "  charactes/NNS\n",
      "  creepy/VBP\n",
      "  happy/JJ\n",
      "  though/IN\n",
      "  asoh/JJ\n",
      "  let/NN\n",
      "  turn/VB\n",
      "  scared/VBN\n",
      "  dead/JJ\n",
      "  bubble/JJ\n",
      "  likable/JJ\n",
      "  graphic/JJ\n",
      "  noob/JJ\n",
      "  thing/NN\n",
      "  music/NN\n",
      "  whats/VBZ\n",
      "  door/JJ\n",
      "  afraid/JJ\n",
      "  class/NN\n",
      "  shine/NN\n",
      "  calmer/NN\n",
      "  look/NN\n",
      "  room/NN\n",
      "  time/NN\n",
      "  hmm/JJ\n",
      "  review/NN\n",
      "  adorable/JJ\n",
      "  around/IN\n",
      "  voice/NN\n",
      "  wait/VBP\n",
      "  child/JJ\n",
      "  return/NN\n",
      "  like/IN\n",
      "  finding/VBG\n",
      "  locked/VBN\n",
      "  menever/NN\n",
      "  kill/NN\n",
      "  beat/VBD\n",
      "  childhood/JJ\n",
      "  write/JJ\n",
      "  odd/JJ\n",
      "  sceme/NN\n",
      "  staring/VBG\n",
      "  chasing/VBG\n",
      "  trying/VBG\n",
      "  game/NN\n",
      "  hearing/VBG\n",
      "  somewhat/RB\n",
      "  friend/JJ\n",
      "  moment/NN\n",
      "  ghost/NN\n",
      "  isnot/JJ\n",
      "  heart/NN\n",
      "  clean/JJ\n",
      "  hello/NN\n",
      "  im/NN\n",
      "  atleast/NN\n",
      "  pause/NN\n",
      "  themor/NN\n",
      "  see/VBP\n",
      "  stand/VBP\n",
      "  bit/RB\n",
      "  1990swhat/CD\n",
      "  ill/JJ\n",
      "  full/JJ\n",
      "  tree/NN\n",
      "  flashlight/NN)\n"
     ]
    }
   ],
   "source": [
    "print(\"Performed Named Entity Recognition\")\n",
    "\n",
    "def named_entity_recognition(text):\n",
    "    result = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "    return result\n",
    "\n",
    "text_1 = df_train.user_review[0]\n",
    "ner = named_entity_recognition(text_1)\n",
    "print(ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed Part-of-speech tagging.\n",
      "0        [(charactes, NNS), (creepy, VBP), (happy, JJ),...\n",
      "1        [(pepper, NN), (youtube, NN), (pantsprosscary,...\n",
      "2        [(control, NN), (casual, JJ), (made, VBN), (ma...\n",
      "3        [(settle, RB), (though, IN), (great, JJ), (yea...\n",
      "4        [(back, RB), (take, JJ), (horror, NN), (late, ...\n",
      "                               ...                        \n",
      "17489    [(high, JJ), (brought, VBD), (greatest, JJS), ...\n",
      "17490    [(consistent, JJ), (restriction, NN), (fairly,...\n",
      "17491    [(loggin, NN), (merged, VBD), (married, JJ), (...\n",
      "17492    [(looping, VBG), (platform, NN), (take, VB), (...\n",
      "17493    [(always, RB), (back, RB), (enjoyed, JJ), (zoo...\n",
      "Name: pos_tagging, Length: 17494, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Performed Part-of-speech tagging.\")\n",
    "\n",
    "# Checking parts of speech tags.\n",
    "def pos_tagging(text):\n",
    "    text = word_tokenize(text)\n",
    "    tokens_tag = pos_tag(text)\n",
    "    return tokens_tag\n",
    "\n",
    "df_train['pos_tagging'] = df_train.user_review.apply(pos_tagging)\n",
    "print(df_train['pos_tagging'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('game', 15258), ('play', 7090), ('like', 6755), ('get', 6647), ('access', 6160), ('early', 5999), ('time', 5505), ('good', 4894), ('fun', 4863), ('dont', 4747), ('one', 4322), ('even', 4172), ('really', 4002), ('make', 3817), ('free', 3623)]\n"
     ]
    }
   ],
   "source": [
    "# Check 15 most frequent terms.\n",
    "\n",
    "token = nltk.word_tokenize(''.join(df_train.user_review))\n",
    "frequent = nltk.FreqDist(token)\n",
    "print(frequent.most_common(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of word 505\n",
      "\n",
      "Sentence:\n",
      " ['buddhist costume war etc aborted sold quiet ad instead underneath icon since ryazan 200 discovered church 8th trench come lenin fly standing satanic unmeltable craft commonly surrounded bubonic crowbar euro trinity ago drunk poison wanted alien signed light weather tube eldeity tell rejecting sky vote june pay wolf turkey biogenetic thru 3 2006 way preach symbol inspired tired logic country third atlantis massive govt energy 1km ruski stripper giant meet zombie new step dont stick bomb volga fight abyss like tsar stadiumsize also another proper zodiac nato radonezh shroud side sarov three cross blue american take coast enoch order dy 3rd water serve taken wear clothes plus weapon natural kelvin food hit base main infirmitiesillnessessicknesses dinosaur italian others red clothed prayer sin inviting bowing creator nerve superpower 2inside normal sinkhole skin planet roman lake schneerson crossing ic stranded ukraine anathema repeatedly slowly priest plague filling mudra nika nine supreme remain created fuel freeze always designed 1moon muslim triple elect life sometimes siberia human woodburning prophet jew elijah grow baikal azazel planetary mercy rocket horoscope exhibit hole bacteriologist demon scandinavia collapse disease metal vyacheslav germany catholic shoulder dusha healed chest worship 2nd right panic israeli orthodox depicted next jesus 10001500 chunk passion fear week lot guardian use people lower learn antichrist trait sample without transfer hence never cortes nuke german pelageya live short acknowledging leg sergiy bread twisted longer basically time destroy chinese devil orthodoxy level want pray nail yosef shake plate scientist ice behind radiation china high per killed switch upside bird result run robe oh groin done flag biggest conquers meeting participate return gaad spirit betraying cell satanist pillar five predicted service vaccine isi stand disagree medical abduction 3in 4in bring surround diamond kneeling implant blasphemy good 7 wont either soldier temple finger trapped synthetic hebrew maya melt god pale inside intelligence showing head stove blame clothing mendel fetus heretic reject appear u back celcius iran saint save eye read spanish hurry angel month look fast blood wound false foot cant creation body krasheninnikov mariana crowned correctly pole clergy shovel russia fully left used say belief bunch put let alexandre old religion together dna christian go ural knife airplane council zero sionists secret mountain kill administered 11th clone crucified soul cut nobody glove one across clothfabrictextile long away slavonic ocean 4 bishop document ww3 dream swallow resurrected santa hide chamber tectonic closet 12 image nanochips earth blow thought need stop super portuguese ufo attack megatsunamis youre october last abducted invite eat kailash deceived holy america care incorrectly allegiance tibet pose language chanted happens patriarch 666 blasphemer 1st forward crowning 8 pacific 2 iv satan xc alaska menachem prison cover possessed higher year he hand 36 try bug 1066 7525 abduct leader keep liberty person youll doesnt sign airspace 666ed communion fake christ statue hexagram holiday believe except first power see flat provides past stay sleep rico saying 2016 allows aggression whole world fall milan earthquake turin seraph medicine eurasia bigger antarctica greenland burned move river word show inland big curse yersin make get many']\n"
     ]
    }
   ],
   "source": [
    "# Text with highest number of words.\n",
    "\n",
    "df_train['number_of_words'] = df_train['user_review'].apply(lambda x: len(str(x).split()))\n",
    "print('Maximum number of word',df_train['number_of_words'].max())\n",
    "print('\\nSentence:\\n',df_train[df_train['number_of_words'] == 505]['user_review'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of target variable\n",
      "1    0.569795\n",
      "0    0.430205\n",
      "Name: user_suggestion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Class distribution of target available.Almost Equal distribution.\n",
    "\n",
    "print(\"Distribution of target variable\")\n",
    "print(df_train.user_suggestion.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robocraft                                             842\n",
      "Eternal Card Game                                     791\n",
      "Heroes & Generals                                     745\n",
      "War Thunder                                           720\n",
      "Fractured Space                                       718\n",
      "Bless Online                                          712\n",
      "The Elder Scrolls®: Legends™                          565\n",
      "Neverwinter                                           546\n",
      "AdventureQuest 3D                                     519\n",
      "theHunter Classic                                     518\n",
      "Creativerse                                           492\n",
      "DCS World Steam Edition                               488\n",
      "Infestation: The New Z                                479\n",
      "Team Fortress 2                                       479\n",
      "PlanetSide 2                                          472\n",
      "Path of Exile                                         458\n",
      "SMITE®                                                454\n",
      "Fallout Shelter                                       447\n",
      "Realm Royale                                          442\n",
      "Trove                                                 430\n",
      "Ring of Elysium                                       419\n",
      "RaceRoom Racing Experience                            416\n",
      "Brawlhalla                                            410\n",
      "Dota 2                                                405\n",
      "Yu-Gi-Oh! Duel Links                                  399\n",
      "Cuisine Royale                                        399\n",
      "Spooky's Jump Scare Mansion                           362\n",
      "Elsword                                               342\n",
      "Realm of the Mad God                                  340\n",
      "World of Tanks Blitz                                  327\n",
      "WARMODE                                               300\n",
      "World of Guns: Gun Disassembly                        293\n",
      "Black Squad                                           288\n",
      "School of Dragons                                     268\n",
      "Bloons TD Battles                                     233\n",
      "Sakura Clicker                                        222\n",
      "Business Tour - Board Game with Online Multiplayer    191\n",
      "Realm Grinder                                         155\n",
      "Crusaders of the Lost Idols                           132\n",
      "EverQuest II                                           69\n",
      "Dreadnought                                            60\n",
      "Freestyle 2: Street Basketball                         57\n",
      "Shop Heroes                                            52\n",
      "Tactical Monsters Rumble Arena                         38\n",
      "Name: title, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Number of user reviews for each video game.\n",
    "# 'Robocraft' game is more popular and has highest number of user reviews.\n",
    "\n",
    "print(df_train.title.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valve                                   884\n",
      "Freejam                                 842\n",
      "Dire Wolf Digital                       791\n",
      "RETO MOTO                               745\n",
      "Gaijin Entertainment                    720\n",
      "Edge Case Games Ltd.                    718\n",
      "NEOWIZ BLESS STUDIO                     712\n",
      "Sparkypants Studios, LLC                565\n",
      "Cryptic Studios                         546\n",
      "Daybreak Game Company                   541\n",
      "Artix Entertainment, LLC                519\n",
      "Expansive Worlds, Avalanche Studios     518\n",
      "Playful Corp.                           492\n",
      "Eagle Dynamics SA                       488\n",
      "Fredaikis AB                            479\n",
      "Grinding Gear Games                     458\n",
      "Titan Forge Games                       454\n",
      "Bethesda Game Studios                   447\n",
      "Heroic Leap Games                       442\n",
      "Trion Worlds                            430\n",
      "Aurora Studio                           419\n",
      "Sector3 Studios                         416\n",
      "Blue Mammoth Games                      410\n",
      "Konami Digital Entertainment            399\n",
      "Darkflow Software                       399\n",
      "Lag Studios                             362\n",
      "KOG                                     342\n",
      "Wild Shadow Studios, Deca Games         340\n",
      "Wargaming Group Limited                 327\n",
      "WARTEAM                                 300\n",
      "Noble Empire Corp.                      293\n",
      "NS STUDIO                               288\n",
      "JumpStart Games, Inc.                   268\n",
      "Ninja Kiwi                              233\n",
      "Winged Cloud                            222\n",
      "Creobit                                 191\n",
      "Divine Games                            155\n",
      "Codename Entertainment Inc.             132\n",
      "Six Foot, YAGER                          60\n",
      "Joycity                                  57\n",
      "Cloudcade, Inc.                          52\n",
      "Camex Games                              38\n",
      "Name: developer, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Who developed maximum number of video games?\n",
    "# 'Valve' developer developed maximum number of games.\n",
    "\n",
    "print(df_train.developer.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year\n",
      "2011.0      14\n",
      "2012.0      65\n",
      "2013.0     340\n",
      "2014.0    1499\n",
      "2015.0    2460\n",
      "2016.0    4226\n",
      "2017.0    3890\n",
      "2018.0    4822\n",
      "Name: review_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Which year has maximum number of reviews published?\n",
    "# '2018' has maximum no of reviews pulished.\n",
    "\n",
    "review_count = df_train.groupby(\"year\")[\"review_id\"].count()\n",
    "print(review_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13995,)\n",
      "(13995,)\n",
      "(3499,)\n",
      "(3499,)\n"
     ]
    }
   ],
   "source": [
    "# Dividing the data into train test split.\n",
    "X = df_train['user_review']\n",
    "y = df_train['user_suggestion']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)\n",
    "\n",
    "# Checking training and validation set data distribution.\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.feature_extraction.text.TfidfVectorizer'>\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(13995, 5000)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(3499, 5000)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(8045, 5000)\n"
     ]
    }
   ],
   "source": [
    "# # Feature Extraction using TFIDF-Char Based.\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,5), stop_words='english', analyzer='char',max_features=5000)\n",
    "print(type(tfidf_vec))  # TfidfVectorizer class.\n",
    "\n",
    "train_tfidf_vec = tfidf_vec.fit_transform(X_train)\n",
    "print(type(train_tfidf_vec))  # Sparse CSR matrix.\n",
    "train_vector_array = train_tfidf_vec.toarray()\n",
    "print(train_vector_array.shape)\n",
    "\n",
    "valid_tfidf_vec = tfidf_vec.transform(X_test)\n",
    "print(type(valid_tfidf_vec))  # Sparse CSR matrix.\n",
    "valid_vector_array = valid_tfidf_vec.toarray()\n",
    "print(valid_vector_array.shape)\n",
    "\n",
    "test_tfidf_vec = tfidf_vec.transform(df_test['user_review'])\n",
    "print(type(test_tfidf_vec))\n",
    "test_vector_array = test_tfidf_vec.toarray()\n",
    "print(test_vector_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.8507751937984496\n",
      "LinearSVC 0.8551219512195122\n",
      "Ridge Classifier 0.8562347188264059\n"
     ]
    }
   ],
   "source": [
    "metric = []\n",
    "model_names = []\n",
    "models = [(\"LogisticRegression\",LogisticRegression(random_state=999)),\n",
    "          (\"LinearSVC\",LinearSVC(C=1,loss= 'hinge',random_state=999)),(\"Ridge Classifier\",RidgeClassifier(random_state=999))]    \n",
    "    \n",
    "for name,model in models:\n",
    "    model.fit(train_vector_array, y_train)\n",
    "    y_pred = model.predict(valid_vector_array)\n",
    "    model_names.append(name)\n",
    "    metric.append(f1_score(y_test,y_pred,average='binary'))\n",
    "\n",
    "for i in range(len(model_names)):\n",
    "    print(model_names[i],metric[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV 1/5] END ......................................alpha=0.1; total time=   3.8s\n",
      "[CV 2/5] END ......................................alpha=0.1; total time=   4.2s\n",
      "[CV 3/5] END ......................................alpha=0.1; total time=   4.1s\n",
      "[CV 4/5] END ......................................alpha=0.1; total time=   4.2s\n",
      "[CV 5/5] END ......................................alpha=0.1; total time=   4.1s\n",
      "[CV 1/5] END ......................................alpha=0.2; total time=   4.1s\n",
      "[CV 2/5] END ......................................alpha=0.2; total time=   4.2s\n",
      "[CV 3/5] END ......................................alpha=0.2; total time=   4.1s\n",
      "[CV 4/5] END ......................................alpha=0.2; total time=   4.2s\n",
      "[CV 5/5] END ......................................alpha=0.2; total time=   4.1s\n",
      "[CV 1/5] END ......................................alpha=0.3; total time=   4.1s\n",
      "[CV 2/5] END ......................................alpha=0.3; total time=   4.0s\n",
      "[CV 3/5] END ......................................alpha=0.3; total time=   4.1s\n",
      "[CV 4/5] END ......................................alpha=0.3; total time=   4.3s\n",
      "[CV 5/5] END ......................................alpha=0.3; total time=   4.2s\n",
      "[CV 1/5] END ......................................alpha=0.4; total time=   4.2s\n",
      "[CV 2/5] END ......................................alpha=0.4; total time=   4.1s\n",
      "[CV 3/5] END ......................................alpha=0.4; total time=   4.1s\n",
      "[CV 4/5] END ......................................alpha=0.4; total time=   3.9s\n",
      "[CV 5/5] END ......................................alpha=0.4; total time=   3.9s\n",
      "[CV 1/5] END ......................................alpha=0.5; total time=   4.1s\n",
      "[CV 2/5] END ......................................alpha=0.5; total time=   4.0s\n",
      "[CV 3/5] END ......................................alpha=0.5; total time=   3.9s\n",
      "[CV 4/5] END ......................................alpha=0.5; total time=   3.9s\n",
      "[CV 5/5] END ......................................alpha=0.5; total time=   4.0s\n",
      "[CV 1/5] END ......................................alpha=0.6; total time=   4.0s\n",
      "[CV 2/5] END ......................................alpha=0.6; total time=   3.9s\n",
      "[CV 3/5] END ......................................alpha=0.6; total time=   3.9s\n",
      "[CV 4/5] END ......................................alpha=0.6; total time=   3.9s\n",
      "[CV 5/5] END ......................................alpha=0.6; total time=   3.9s\n",
      "[CV 1/5] END ......................................alpha=0.7; total time=   3.9s\n",
      "[CV 2/5] END ......................................alpha=0.7; total time=   3.9s\n",
      "[CV 3/5] END ......................................alpha=0.7; total time=   3.9s\n",
      "[CV 4/5] END ......................................alpha=0.7; total time=   3.9s\n",
      "[CV 5/5] END ......................................alpha=0.7; total time=   3.9s\n",
      "[CV 1/5] END ......................................alpha=0.8; total time=   3.9s\n",
      "[CV 2/5] END ......................................alpha=0.8; total time=   3.9s\n",
      "[CV 3/5] END ......................................alpha=0.8; total time=   3.9s\n",
      "[CV 4/5] END ......................................alpha=0.8; total time=   3.9s\n",
      "[CV 5/5] END ......................................alpha=0.8; total time=   3.9s\n",
      "[CV 1/5] END ......................................alpha=0.9; total time=   4.0s\n",
      "[CV 2/5] END ......................................alpha=0.9; total time=   4.2s\n",
      "[CV 3/5] END ......................................alpha=0.9; total time=   4.3s\n",
      "[CV 4/5] END ......................................alpha=0.9; total time=   4.2s\n",
      "[CV 5/5] END ......................................alpha=0.9; total time=   4.2s\n",
      "[CV 1/5] END ......................................alpha=1.0; total time=   4.2s\n",
      "[CV 2/5] END ......................................alpha=1.0; total time=   4.2s\n",
      "[CV 3/5] END ......................................alpha=1.0; total time=   4.2s\n",
      "[CV 4/5] END ......................................alpha=1.0; total time=   4.3s\n",
      "[CV 5/5] END ......................................alpha=1.0; total time=   4.2s\n",
      "{'alpha': 0.9}\n",
      "RidgeClassifier(alpha=0.9, random_state=999)\n",
      "0.85595703125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# defining parameter range\n",
    "param_grid = {'alpha': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]} \n",
    "\n",
    "model = RidgeClassifier(random_state=999)\n",
    "grid = GridSearchCV(model, param_grid = param_grid, refit = True,scoring='f1',verbose = 3)\n",
    "  \n",
    "# fitting the model for grid search.\n",
    "grid.fit(train_vector_array,y_train)\n",
    "\n",
    "# print best parameter after tuning.\n",
    "print(grid.best_params_)\n",
    "  \n",
    "# print how our model looks after hyper-parameter tuning\n",
    "print(grid.best_estimator_)\n",
    "y_pred= grid.predict(valid_vector_array)\n",
    "# y_pred = y_pred.reshape(-1, 1)\n",
    "# y_pred = target_scaler.inverse_transform(y_pred)\n",
    "print(f1_score(y_test,y_pred,average='binary'))\n",
    "\n",
    "pred_final = grid.predict((test_vector_array))\n",
    "df = pd.DataFrame({'review_id': df_test['review_id'], 'user_suggestion': pred_final})\n",
    "df.to_csv(\"submission_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Topic Modelling -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 18381)\t1\n",
      "  (0, 18418)\t1\n",
      "  (0, 15271)\t1\n",
      "  (0, 19380)\t1\n",
      "  (0, 18564)\t1\n",
      "  (0, 18610)\t1\n",
      "  (0, 9291)\t1\n",
      "  (0, 13749)\t1\n",
      "  (0, 2356)\t1\n",
      "  (0, 5511)\t1\n",
      "  (0, 2951)\t1\n",
      "  (0, 796)\t1\n",
      "  (0, 45)\t1\n",
      "  (0, 2814)\t1\n",
      "  (0, 1324)\t1\n",
      "  (0, 7991)\t1\n",
      "  (0, 8960)\t1\n",
      "  (0, 8314)\t1\n",
      "  (1, 4448)\t1\n",
      "  (1, 3840)\t1\n",
      "  (1, 8026)\t1\n",
      "  (1, 11871)\t1\n",
      "  (1, 12302)\t1\n",
      "  (1, 13810)\t1\n",
      "  (1, 1429)\t1\n",
      "  :\t:\n",
      "  (13994, 1947)\t1\n",
      "  (13994, 2846)\t1\n",
      "  (13994, 5614)\t1\n",
      "  (13994, 10797)\t1\n",
      "  (13994, 6969)\t1\n",
      "  (13994, 1531)\t1\n",
      "  (13994, 16944)\t1\n",
      "  (13994, 16971)\t1\n",
      "  (13994, 12280)\t1\n",
      "  (13994, 16448)\t1\n",
      "  (13994, 8241)\t1\n",
      "  (13994, 14365)\t1\n",
      "  (13994, 7560)\t1\n",
      "  (13994, 12697)\t1\n",
      "  (13994, 11525)\t1\n",
      "  (13994, 19446)\t1\n",
      "  (13994, 19396)\t1\n",
      "  (13994, 15815)\t1\n",
      "  (13994, 11800)\t1\n",
      "  (13994, 13412)\t1\n",
      "  (13994, 19397)\t1\n",
      "  (13994, 4392)\t1\n",
      "  (13994, 16917)\t1\n",
      "  (13994, 15690)\t1\n",
      "  (13994, 4022)\t1\n"
     ]
    }
   ],
   "source": [
    "# LDA Topic Modelling:\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(max_df=0.8, min_df=2, stop_words='english')\n",
    "doc_term_matrix = count_vect.fit_transform(X_train)\n",
    "print(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.decomposition._lda.LatentDirichletAllocation'>\n",
      "[[0.37825763 5.29821658 0.33337756 ... 0.33394488 0.33385753 4.33263129]\n",
      " [3.06458318 0.53466473 0.36295541 ... 2.33233438 2.32904247 0.33380535]\n",
      " [2.55715919 6.1671187  2.30366703 ... 0.33372074 0.3371     0.33356336]]\n",
      "(3, 19641)\n"
     ]
    }
   ],
   "source": [
    "# Each of x documents is represented as y dimensional vector,which means that our vocabulary has y words.\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "LDA = LatentDirichletAllocation(n_components=3, random_state=0)\n",
    "print(type(LDA))  # LatentDirichletAllocation class.\n",
    "LDA.fit(doc_term_matrix)   # Training the topic model.\n",
    "print(LDA.components_)   # Topic model matrix estimated.\n",
    "print(LDA.components_.shape)\n",
    "\n",
    "# Each topic represents the probability distributions of words in topics.\n",
    "# For each topic,each word of the document is assigned a weight.\n",
    "# Each cell represents probability that the word( in column) belongs to the topic(in row).\n",
    "# Higher weight means it is the top word of the topic.\n",
    "# It is a multidimensional array.Each row represent the topic,each column represents the word in a document.\n",
    "# Shape = [n_topics,n_words] or [n_components, n_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model: \n",
      "\n",
      "Topic #0:\n",
      "[('play', 1873.9701028159193), ('like', 1819.8517918374307), ('fun', 1496.3040589083046), ('good', 1424.0696876292977), ('time', 1385.2889684432998), ('player', 1284.4578320714174), ('free', 1074.980272394731), ('dont', 1072.4473743523579), ('make', 1042.1756908353773), ('weapon', 1033.241152626176), ('new', 998.039070778905), ('really', 965.1657076404845), ('great', 963.4302558854909), ('lot', 928.1711654188557), ('playing', 924.6906898940615), ('thing', 914.1581767955669), ('team', 902.7756568872375), ('people', 881.1657605643267), ('want', 867.6781247958198), ('access', 846.2590779452757)]\n",
      "======================================================================\n",
      "\n",
      "Topic #1:\n",
      "[('access', 3766.0386898840948), ('early', 3742.6721500228164), ('play', 1507.6524616748188), ('like', 1429.322209135684), ('good', 1104.0717394877656), ('card', 1065.6210376854383), ('fun', 976.4997368467582), ('time', 837.1273333170902), ('dont', 771.5148997026128), ('really', 758.5166373832841), ('free', 674.4972673740853), ('player', 670.4671268769735), ('make', 663.6686644875223), ('playing', 651.120850712055), ('played', 609.8598134742697), ('great', 580.751611901004), ('deck', 579.3186861318754), ('reviewthis', 539.7928051049712), ('reviewi', 532.0286488371515), ('better', 518.0283048074534)]\n",
      "======================================================================\n",
      "\n",
      "Topic #2:\n",
      "[('play', 2340.377435509116), ('time', 2259.583698239458), ('like', 2254.8259990267356), ('dont', 1982.0377259448808), ('money', 1469.9634379806232), ('really', 1456.3176549760828), ('good', 1451.858572882791), ('fun', 1394.1962042447951), ('make', 1393.155644676952), ('hour', 1334.4570297940263), ('thing', 1333.403959555497), ('want', 1318.250266836511), ('playing', 1292.1884593937389), ('people', 1222.8436037777608), ('played', 1173.9585088345461), ('free', 1168.522460231037), ('pay', 1168.1958717105579), ('im', 1035.5400408379962), ('say', 1010.5735072316835), ('new', 1006.7880278180671)]\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Define helper function to print top 20 words for each topic.\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for index, topic in enumerate(model.components_):\n",
    "        message = \"\\nTopic #{}:\".format(index)\n",
    "        print(message)\n",
    "        print([(feature_names[i], topic[i]) for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        # feature_names[i] is a word,topic[i] is the weight of the word for that topic.\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "number_of_words = 20\n",
    "print(\"\\nTopics in LDA model: \")\n",
    "tf_feature_names = count_vect.get_feature_names()\n",
    "print_top_words(LDA, tf_feature_names, number_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8314)\t0.2360154637717765\n",
      "  (0, 8960)\t0.28067186752862744\n",
      "  (0, 7991)\t0.33332232087877267\n",
      "  (0, 1324)\t0.3437904892516539\n",
      "  (0, 2814)\t0.14808285148645917\n",
      "  (0, 45)\t0.17821202744872022\n",
      "  (0, 796)\t0.1612218305305089\n",
      "  (0, 2951)\t0.16187018578703283\n",
      "  (0, 5511)\t0.13760069042888848\n",
      "  (0, 2356)\t0.10509936685331153\n",
      "  (0, 13749)\t0.09033386043274579\n",
      "  (0, 9291)\t0.3437904892516539\n",
      "  (0, 18610)\t0.28522630098134516\n",
      "  (0, 18564)\t0.17821202744872022\n",
      "  (0, 19380)\t0.20955963188948884\n",
      "  (0, 15271)\t0.2965122110955658\n",
      "  (0, 18418)\t0.33332232087877267\n",
      "  (0, 18381)\t0.1340068532695946\n",
      "  (1, 16)\t0.13852512083288648\n",
      "  (1, 260)\t0.283207067945484\n",
      "  (1, 18609)\t0.22692588593797738\n",
      "  (1, 13333)\t0.3337102176610715\n",
      "  (1, 11798)\t0.2733004351761694\n",
      "  (1, 12059)\t0.1757013201370694\n",
      "  (1, 3263)\t0.21869967632646073\n",
      "  :\t:\n",
      "  (13994, 1531)\t0.14335378367820664\n",
      "  (13994, 6969)\t0.13907160818600603\n",
      "  (13994, 10797)\t0.13139879093856846\n",
      "  (13994, 5614)\t0.19131892185155172\n",
      "  (13994, 2846)\t0.1767456460095546\n",
      "  (13994, 1947)\t0.09012544803732962\n",
      "  (13994, 5696)\t0.13855665706669848\n",
      "  (13994, 13536)\t0.10992325456276282\n",
      "  (13994, 18676)\t0.1276282469721212\n",
      "  (13994, 9621)\t0.08431553334566774\n",
      "  (13994, 13715)\t0.10823554524466523\n",
      "  (13994, 588)\t0.11828465710012658\n",
      "  (13994, 11336)\t0.09944719986585628\n",
      "  (13994, 9916)\t0.07886117854576909\n",
      "  (13994, 525)\t0.10823554524466523\n",
      "  (13994, 9953)\t0.09065438411920052\n",
      "  (13994, 9782)\t0.10375470349200384\n",
      "  (13994, 19345)\t0.09204946102255705\n",
      "  (13994, 14933)\t0.08248140589073999\n",
      "  (13994, 1802)\t0.10531174945170857\n",
      "  (13994, 14853)\t0.10789905813437406\n",
      "  (13994, 10146)\t0.07599978605442709\n",
      "  (13994, 5484)\t0.047331508695618835\n",
      "  (13994, 837)\t0.04675433753824431\n",
      "  (13994, 17482)\t0.049042808770737835\n"
     ]
    }
   ],
   "source": [
    "# Topic Modelling : NMF: Non-Matrix factorization.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer(max_df=0.8, min_df=2, stop_words='english')\n",
    "doc_term_matrix = tfidf_vect.fit_transform(X_train)\n",
    "print(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.decomposition._nmf.NMF'>\n",
      "[[2.40688737e-03 4.59567308e-03 2.51491525e-04 ... 0.00000000e+00\n",
      "  0.00000000e+00 1.87927372e-04]\n",
      " [0.00000000e+00 1.57029215e-04 5.38607894e-04 ... 8.29197752e-03\n",
      "  2.67162992e-03 0.00000000e+00]\n",
      " [8.50182892e-05 1.03301199e-03 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 9.52366204e-04]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf = NMF(n_components=3, random_state=42)\n",
    "print(type(nmf))\n",
    "nmf.fit(doc_term_matrix)\n",
    "print(nmf.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in NMF model: \n",
      "\n",
      "Topic #0:\n",
      "[('play', 0.682134903954845), ('time', 0.6569724237589522), ('dont', 0.6170149421688628), ('like', 0.6110676711263189), ('fun', 0.5548207747195675), ('good', 0.5184667186978327), ('really', 0.5167289604579098), ('money', 0.5071019838977018), ('want', 0.4964738312601318), ('pay', 0.48815659736460126), ('thing', 0.48574028840280264), ('free', 0.4791915582789945), ('playing', 0.47333649095176644), ('people', 0.470235982486659), ('make', 0.4674733601900885), ('hour', 0.46600545608471844), ('player', 0.41423125744903544), ('new', 0.4076184116426337), ('played', 0.39252152176098787), ('need', 0.38794884914687505)]\n",
      "======================================================================\n",
      "\n",
      "Topic #1:\n",
      "[('access', 1.7716563877526221), ('early', 1.7494482743617519), ('reviewthis', 0.6275718059756573), ('reviewi', 0.5006410557870654), ('good', 0.35948582836993526), ('fun', 0.3272890251367439), ('like', 0.319767817885841), ('pubg', 0.26065666373276525), ('play', 0.2506117539327672), ('royale', 0.24769700942406428), ('reviewthe', 0.24399721394770132), ('great', 0.22533387886679293), ('battle', 0.2201698912723781), ('update', 0.20817988359622688), ('really', 0.20510419247612793), ('better', 0.18675176928127105), ('recommend', 0.1786546031616459), ('graphic', 0.17760892369482656), ('fortnite', 0.1741307158710297), ('alpha', 0.16951472160257822)]\n",
      "======================================================================\n",
      "\n",
      "Topic #2:\n",
      "[('card', 1.045130562201233), ('hearthstone', 0.6736684543821271), ('deck', 0.6605610681096507), ('magic', 0.46944875344664044), ('mtg', 0.37435788173093465), ('mechanic', 0.2961898748677156), ('generous', 0.28049328093350007), ('gathering', 0.27866912692910256), ('elder', 0.2501923451482573), ('pack', 0.24846059337582313), ('scroll', 0.2436611792042249), ('eternal', 0.23402713680458445), ('mana', 0.22495269340272334), ('mode', 0.2173764353547724), ('ccg', 0.21401873542223696), ('player', 0.2074968950640109), ('digital', 0.20468728270840336), ('reward', 0.20411309670969993), ('play', 0.1973343367943905), ('f2p', 0.19588919175640673)]\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Define helper function to print 20 top words.\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for index, topic in enumerate(model.components_):\n",
    "        message = \"\\nTopic #{}:\".format(index)\n",
    "        print(message)\n",
    "        print([(feature_names[i], topic[i]) for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "number_of_words = 20\n",
    "print(\"\\nTopics in NMF model: \")\n",
    "tf_feature_names = tfidf_vect.get_feature_names()  # note that tf_vectorizer is an LemmaCountVectorizer object and with this command we get the whole dictionary of words\n",
    "print_top_words(nmf, tf_feature_names, number_of_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
